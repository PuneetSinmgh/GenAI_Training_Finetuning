# Retriever.py
# This code demonstrates how to use Huggingface's DPR (Dense Passage Retrieval) models to
# encode text and visualize embeddings using t-SNE. It includes functions to read and split text,
# tokenize input, create a context encoder, and generate embeddings for given paragraphs.

import numpy as np
import random
from transformers import DPRContextEncoder, DPRQuestionEncoderTokenizer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.manifold import TSNE
import numpy as np


# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# Retriever is comparise of tokenizer, encoder and aggregation function
# tokenize the input text and context
text = [("How are you?", "I am fine."), ("What's up?", "Not much.")]
print(text)

context_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')
context_tokenizer

tokens_info=context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)
print (tokens_info.keys())

for s in tokens_info['input_ids']:
   print(context_tokenizer.convert_ids_to_tokens(s))

# create the context encoder model
from DataHelper import read_and_split_text
filename = 'companyPolicies.txt'

paragraphs =  read_and_split_text(filename)

context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')
random.seed(42)
random.shuffle(paragraphs)

tokens=context_tokenizer( paragraphs[:20], return_tensors='pt', padding=True, truncation=True, max_length=256) 
tokens
outputs=context_encoder(**tokens)
outputs.pooler_output

def encode_contexts(text_list):
    # Encode a list of texts into embeddings
    embeddings = []
    for text in text_list:
        inputs = context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)
        outputs = context_encoder(**inputs)
        embeddings.append(outputs.pooler_output)
    return torch.cat(embeddings).detach().numpy()

# you would now encode these paragraphs to create embeddings.
context_embeddings = encode_contexts(paragraphs)

