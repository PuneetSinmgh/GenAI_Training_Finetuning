# Retriever.py
# This code demonstrates how to use Huggingface's DPR (Dense Passage Retrieval) models to
# encode text and visualize embeddings using t-SNE. It includes functions to read and split text,
# tokenize input, create a context encoder, and generate embeddings for given paragraphs.

import numpy as np
import random
from transformers import DPRContextEncoder, DPRQuestionEncoderTokenizer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.manifold import TSNE
import numpy as np
import faiss



# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# Retriever is comparise of tokenizer, encoder and aggregation function
# tokenize the input text and context
text = [("How are you?", "I am fine."), ("What's up?", "Not much.")]
print(text)

context_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')
context_tokenizer

tokens_info=context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)
print (tokens_info.keys())

for s in tokens_info['input_ids']:
   print(context_tokenizer.convert_ids_to_tokens(s))

# create the context encoder model
from DataHelper import read_and_split_text
filename = 'companyPolicies.txt'

paragraphs =  read_and_split_text(filename)

context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')
random.seed(42)
random.shuffle(paragraphs)

tokens=context_tokenizer( paragraphs[:20], return_tensors='pt', padding=True, truncation=True, max_length=256) 
tokens
outputs=context_encoder(**tokens)
outputs.pooler_output

def encode_contexts(text_list):
    # Encode a list of texts into embeddings
    embeddings = []
    for text in text_list:
        inputs = context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256) # tokenize each text
        outputs = context_encoder(**inputs)   # encoding
        embeddings.append(outputs.pooler_output) # aggregation
    return torch.cat(embeddings).detach().numpy() 

# you would now encode these paragraphs to create embeddings.
context_embeddings = encode_contexts(paragraphs)

# generate FAISS index for the embeddings

def create_faiss_index(context_embeddings):
    # Convert list of numpy arrays into a single numpy array
    embedding_dim = 768  # This should match the dimension of your embeddings
    context_embeddings_np = np.array(context_embeddings).astype('float32')

    # Create a FAISS index for the embeddings
    index = faiss.IndexFlatL2(embedding_dim)
    index.add(context_embeddings_np)  # Add the context embeddings to the index
    return index
